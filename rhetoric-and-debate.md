# Rhetoric and Debate

---

* [Good- and Bad-Faith Debate](rhetoric-and-debate#good--and-bad-faith-debate)
* [Valid Argumentative Techniques](rhetoric-and-debate#valid-argumentative-techniques)
   * [Logical Steps](rhetoric-and-debate#logical-steps)
   * [Good-Faith Debate Tactics](rhetoric-and-debate#good-faith-debate-tactics)
* [Invalid Argumentative Techniques](rhetoric-and-debate#invalid-argumentative-techniques)
   * [Logical and Statistical Fallacies](rhetoric-and-debate#logical-and-statistical-fallacies)
   * [Bad-Faith Debate Tactics](rhetoric-and-debate#bad-faith-debate-tactics)
* [Substantiating Your Points](rhetoric-and-debate#substantiating-your-points)
   * [Appeals to Authority](rhetoric-and-debate#appeals-to-authority)
   * [Selection Bias](rhetoric-and-debate#selection-bias)
   * [Anecdotal Evidence and the Need for Data](rhetoric-and-debate#anecdotal-evidence-and-the-need-for-data)
   * [The Texas Sharpshooter](rhetoric-and-debate#the-texas-sharpshooter)
   * [Why It's Important to Contextualize Data](rhetoric-and-debate#why-its-important-to-contextualize-data)
   * [Correlation and Causation](rhetoric-and-debate#correlation-and-causation)
* [The Pyschology of Changing Minds](rhetoric-and-debate#the-pyschology-of-changing-minds)
   * [The Propagation of Misinformation](rhetoric-and-debate#the-propagation-of-misinformation)
   * [The Persistance of Misinformation](rhetoric-and-debate#the-persistance-of-misinformation)
   * [Tactics for Debunking Myths and Changing Minds](rhetoric-and-debate#tactics-for-debunking-myths-and-changing-minds)

---

## Good- and Bad-Faith Debate

The following quote comes from John Reed, author of *How to Spot Dishonest Arguments* ([source](https://johntreed.com/blogs/john-t-reed-s-news-blog/60887299-intellectually-honest-and-intellectually-dishonest-debate-tactics)), and is worth considering: 

“There are only two intellectually-honest debate tactics:
1. pointing out errors or omissions in your opponent’s facts, and
2. pointing out errors or omissions in your opponent’s logic.” 

Similar is Paul Graham’s [Pyramid of Disagreement](http://www.paulgraham.com/disagree.html), which argues that there is a “hierarchy of disagreement”, where the higher-up forms of disagreement are more intellectually honest. Wikipedia contains [this helpful diagram](https://en.wikipedia.org/wiki/Paul_Graham_(programmer)#/media/File:Graham's_Hierarchy_of_Disagreement-en.svg) which summarizes Graham's points. The pyramid might seem obvious, but being conscious about your aims can help you stay focused in the heat of a debate.

<p align="center">
  <img width="600" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Graham%27s_Hierarchy_of_Disagreement-en.svg/1280px-Graham%27s_Hierarchy_of_Disagreement-en.svg.png">
</p>

In short, a bad-faith actor is someone who does not enter a debate to learn or participate in a Socratic dialogue. It is someone who enters to “win” (usually, this means making themselves appear strong to the audience) and relies on invalid argumentative techniques (as discussed later) to do so. One red flag of a bad-faith actor is a lack of a consistent ideology; obvious contradictions usually means that the actor hasn’t actually grappled with critique. Yet it’s important to remember that calling someone a bad-faith actor to put them down instead of trying to better the conversation is itself bad-faith acting.

## Valid Argumentative Techniques

### Logical Steps

When trying to prove a statement *deductively* (with pure logic), one should rely on the [rules of inference](https://en.wikipedia.org/wiki/List_of_rules_of_inference), the [rules of replacement](https://en.wikipedia.org/wiki/Rule_of_replacement), and the [valid syllogistic forms](http://markmcintire.com/phil/validforms.html#). There is no need to list them out here: virtually all valid rules or forms are immediately intuitive, and almost all invalid rules or forms are obviously invalid (though some find the ideas of [existential import](http://cstl-cla.semo.edu/hhill/pl120/notes/existential%20import.htm) and the [existential fallacy](https://en.wikipedia.org/wiki/Existential_fallacy) genuinely confusing). One interesting technique in pure logic is proof by contradiction. This argumentative technique is also called *[reductio ad absurdum](https://iep.utm.edu/reductio/)*, and begins with the acceptance of a position, and the derivation of something absurd (in the logical sense). 

There is a fabulous quote by G.H. Hardy on this technique: “Reductio ad absurdum... is a far finer gambit then any chess gambit: a chess player may offer the sacrifice of a pawn or even a piece, but a mathematician offers the game” ([source](http://web.mnstate.edu/peil/geometry/Logic/6logic.htm)).

However, not all statements need to be proven deductively. *Inductive arguments* are arguments that do not prove their conclusion with total certainty, but rather argue the conclusion is extremely likely. Inductive arguments are not judged by their logical consistency, but by how "strong" or "weak" they are. For example, when done correctly, an argument from analogy can serve as inductive justification for a particular position. Of course, creating a false analogy is a fallacy, and determining what is “false” is hard, but this doesn’t mean that arguments from analogy are worthless. Another example of strong inductive reasoning is the process of using a large random sample of a population to reason about the entire population. [Here](https://examples.yourdictionary.com/examples-of-inductive-reasoning.html) are more examples of inductive reasoning.

### Good-Faith Debate Tactics

Here is a list of tactics that might not help you make your opponent “look bad”, but will ultimately help you convince them to change their mind. 
* *Stay calm.* Stereotypes about the overemotional abound; these stereotypes are undoubtedly harmful, but for the sake of optics it’s important to avoid them. 
* *Concede good points and point out when your opponent gets something right.* Dogmatically refusing to admit nuance in your conversation partner’s position (even when, to you, it seems like they have none) will only lock away the possibility for
* *Accept small changes.* You don’t need to compromise, but be prepared to accept small steps by your partner in the right direction. Ideology change does not happen all at once. 
* *Study your partner.* On one hand, understanding your opponent’s positions and tendencies will be helpful to prepare for their arguments or catch them in logical inconsistencies. On the other, it can help educate you on what your partner is missing or what arguments will be most effective.
* *Require a consistent ideology.* Tu quoque (discussed later) is a fallacy, but it is absolutely reasonable to require that each point of your partner’s ideology agrees with earlier points. 
* *Steel man your partner’s position.* Instead of trying to find a weak version of your opponent’s argument, find a strong version. Make their argument as effective as possible and restate it to them so that (1) you’re sure you understand them and (2) they know you’re listening to them. 

And, of course, avoid the fallacies and bad-faith debate tactics discussed later in this chapter. 

## Invalid Argumentative Techniques

### Logical and Statistical Fallacies

*Note:* I created this list myself, but used [this](https://yourlogicalfallacyis.com/), [this](https://www.steelonsteel.com/wp-content/uploads/2019/09/FOTW-2019-List-min.jpg), and [this site](https://thevisualcommunicationguy.com/wp-content/uploads/2015/02/Infographic_LogicalFallaciesCollection_LowRes.jpg) to make sure I didn’t miss any fallacies. 

*Argumentum ad hominem* (*ad hominem* for short) is a logical fallacy where the speaker tries to invalidate an argument by attacking the attributes of the argument’s maker rather than dealing with the argument itself. There are a few specific types of ad hominem arguments: 
* *Tu quoque* literally means “you also”, and refers to when the speaker tries to disprove an argument by attacking the history of the arguer. For example, a tu quoque fallacy occurs when the speaker tries to invalidate an argument on the basis that the arguer is a hypocrite.
* *Circumstantial ad hominem* is the fallacy of invalidating an argument on the basis that the arguer is biased. If properly justified, it is reasonable to invalidate empirical evidence due to the collector's bias, but the bias of the arguer never justifies the invalidation of an argument. 
* *Guilt by association*, *name-calling*, *the genetic fallacy*, and so on are all ad hominem fallacies. 

*Argumentum ad populum* (*ad populum* for short) is the logical fallacy of assuming that a proposition is true if most (or many) believe it, or conversely that a proposition is false if most (or many) don’t believe it. Ad populum is also called *appeal to popularity* or *the democratic fallacy*. On the other hand, there are times when appealing to popularity is valid; for example, elected officials in a representative democracy might look at polls to help determine what policies to implement. 

Related is the fallacious *appeal to authority*, where a speaker argues that a proposition is true solely because someone in a position of authority said so. However, despite the fact that this is a logical fallacy, the truth is that relying on academic evidence is often not just compelling, but necessary, to agree on certain base premises. In the section on substantiating points, we discuss appeals to authority — including how and why they can be done well — in more detail. 

*Affirming the consequent* is the logical fallacy of assuming that an implication implies its converse. More explicitly, this logical fallacy occurs when *A* implies *B*, and the speaker uses this to argue that if *B*, then *A*. For example, it is true that all apples are fruits, but would be fallacious to assume that any fruit necessarily be an apple. Equivalent is the logical fallacy of denying the antecedent.

Similar is the logical fallacy of *begging the question* or *circular reasoning*, where the speaker assumes a restated version of their proposition to prove it. For example, the statement “abortion is wrong because it is murder” begs the question because murder means “wrongful killing”. 

*The naturalistic fallacy* (also called *appeal to nature* or *appeal to tradition*) is the fallacy of assuming that because something is a certain way (or was a certain way, or has been a certain way) that it should be that way. Related is the *moralistic fallacy*, which essentially holds that some aspect of nature is undesirable or immoral and thus cannot exist. For example, I have heard on multiple occasions the argument that “X cannot exist, because God would not let such an awful thing happen.”

*Argumentum ad ignorantiam*, *the appeal to ignorance fallacy*, occurs when someone asserts that a proposition is true because they have seen (or there exists) no evidence to the contrary. This fallaciously shifts the burden of proof from the one making the claim to the one critiquing it, and thus is also called *the burden of proof fallacy*. In general, the burden of proof lies on the person making the claim (this is also known as *Russell’s teapot*). 

*The fallacy of composition* is related to *the fallacy of hasty generalization* and is where one concludes that something is true of an entire group from the fact that it’s true of some part of the whole. It is the converse of *the fallacy of division*, which is where one concludes that something is true of some part of a group from the fact that it applies to the whole group; it is essentially the problem with applying averages to individuals. For example, assuming that a particular person is more likely to be a criminal because they are part of a group which is more likely to be criminals. 

Related is *the ecological fallacy*, which generally happens when one confuses ecological correlations and individual correlations. This can manifest itself in using averages wrongly, in subsets’ behavior being contrary to the larger set’s behavior, and so on. One of the most famous examples of this was [Robinson’s paradox](https://www.jstor.org/stable/2087176), where Robinson found that there was a negative correlation between immigrants in an American state and its illiteracy rate. However, upon closer consideration of the data, he found that immigrants actually had a slightly higher illiteracy rate than the native population; they simply tended to move to states with high literacy rates. 

Another statistics-related fallacy is the confusion of *correlation vs. causation* (also known as non causa pro causa). In the above entry, we saw confusion between two types of correlations; in this fallacy, we note that just because *A* and *B* are correlated does not mean *A* causes *B*. It could be the case that *B* causes *A*, or that *A* and *B* are both caused by *C*, etc. For example, it is true that intelligence and wealth are correlated, but this does not necessarily mean intelligence causes wealth; an equally important realization is that having wealth (or, more accurately, not having to worry about poverty) actually makes one more intelligent (see the section on Class and Intelligence). 

A *false dichotomy* (also called a *black-and-white fallacy* or an *excluded middle fallacy*) occurs when a speaker tries to fallaciously cast a decision as having only two possible choices, when there may be a myriad of other possibilities. Conversely, the *middle ground fallacy* occurs when one assumes the compromise of two extremes must be the correct answer. 

The *slippery slope fallacy* occurs when one asserts that a small step leads to a chain of events leading to one a large (negative) event without sufficient justification. This is related to the *continuum fallacy*, which occurs when one ignores the possibility of a middle ground between two objects *A* and *B*. 

Finally, the *straw man fallacy* occurs when someone misrepresents their opponent’s argument in order to have an easier time refuting it. It is often used in conjunction with the slippery slope argument, wherein someone argues that an argument for *A* is really an argument for *B*, where the connection between *A* and *B* is provided by slippery-slope reasoning. 

Other important fallacies: 
* *appeal to emotion*, when an argument relies on the speaker’s emotions rather than logic
* *personal incredulity*, when a speaker invalidates a proposition due to personal disbelief
* *the gambler’s fallacy*, when a speaker tries to find a pattern in statistically independent events
* *the fallacy fallacy*, which is when a proposition is invalidated because a common argument for the proposition has a fallacy in it. 
* *loaded question/definition*, when a speaker defines something or asks a question in a biased manner. This is related to the straw man fallacy.

### Bad-Faith Debate Tactics

*The Gish gallop* is a bad-faith debate tactic where the arguer, instead of trying to have a two-way discussion, will try to overwhelm their opponent with point after point. Often, each of these points could be refuted if they were given individually, but together, it is difficult for the opponent to remember all of them, much less actually respond to them. 

*The Motte-and-Bailey tactic* occurs when an arguer holds a fairly controversial position (the “bailey”), and, upon being challenged, retreats to a much easier-to-defend position (the “motte”). By conflating the bailey with the motte, if their opponent cannot reject the motte, they can claim that they have proven the bailey. For example, a famous anti-intellectual motte-and-bailey argument uses the bailey “scientific knowledge cannot be trusted” but the motte “scientists have been wrong in the past”. 

Related is the tactic of *moving the goalposts*. Moving the goalposts occurs when a speaker holds a position justified by a fallacious argument, and when said argument is disproven, refuses to accept it, instead creating a new argument that justifies an easier-to-defend position. The new argument is often fallacious itself, and so the cycle can continue. The reason why this is considered a bad-faith tactic is because, if the speaker was trying to have a Socratic dialogue, they would begin with their actual position and the strongest argument for it. 

*Pivoting*, *a non sequitur*, or *a red herring* is a debate tactic/logical fallacy wherein the speaker will ignore the argument or proposition at hand, and make an entirely irrelevant point. Sometimes, this takes the form of ignoring an opponent’s argument and simply throwing out fallacious critiques of their conclusion. Other times, the speaker will simply change the subject instead of admitting defeat (presumably to insulate themselves from having to admit to being wrong). 

## Substantiating Your Points
### Appeals to Authority
Appealing to an authority is often a good heuristic when a proper argument cannot be made; but for an educated and substantiated position, one cannot simply rely on other people's positions. Furthermore, what an "authority" is can be misleading; there is precedent for people to lie about their their qualifications (see [this article](https://skepticalscience.com/consensus-and-arguments-from-authority.html)). Finally, relying too much on authority can lead people to lean into ["The Courtier's Reply"](https://en.wikipedia.org/wiki/Courtier%27s_reply), which is essentially when someone's argument is ignored because they are arguing against authority from a position of no authority. Your argument, whether it comes from a place of authority or not, should stand on its own two feet.

However, there needs to be an important distinction between quoting an authority and relying on scientific evidence; these are not comparable. Carl Sagan once said that “arguments from authority carry little weight – authorities have made mistakes in the past. They will do so again in the future. Perhaps a better way to say it is that in science there are no authorities; at most, there are experts” [(source)](https://www.goodreads.com/quotes/535475-arguments-from-authority-carry-little-weight-authorities-have-made). The [scientific method](https://plato.stanford.edu/entries/scientific-method/) requires that any theory which is adopted has overwhelming evidence to support it, and that all reasonable alternative hypotheses have been falsified. Scientific theories are not "true" (in the context of science, "true" means "is the best model we know to predict observable reality") because scientists agree with them. They are true because they are validated by empirical evidence and rigorous argumentation on that evidence. 

Good arguments (as the one made by Carl Sagan above) are not true *because* they are made by an authority; one quotes good arguments made by an authority because the arguments are good, not because they were made by an authority. Studies are themselves arguments: they're just empirical reasoning rather than formal logic. One doesn't value the results of a study because it was made by scientists, but because it's rigorous empirical arguments. This, perhaps more than anything else, underscores the necessity of reading at least part of the studies you cite. You are implicitly making an empirical argument; instead of skipping to the conclusion, it's better to get an understanding of the reasoning wnich got us there. 

With all this said, since each fragment of knowledge is built on another, relying on academic consensus is often necessary for timeliness. Even consensus isn't *why* something is true, the honest truth is it's still sometimes useful to convince yourself (or others) of a secondary fact. Luckily, academic consensus, despite the flaws of the humans which compose it, has proven to be quite reliable in almost every field: much more reliable than anecdotally chosing to listen a single person with fancy credentials. The difference between citing a single authority and academic consensus is as significant and profound as the difference between citing a single data point and a massive dataset. 

The argument I make here is discussed in more detail [here](https://scienceornot.net/2012/01/31/science-is-built-on-the-contributions-of-scientists-not-on-their-authority/).

### Selection Bias

*Selection bias* occurs when, for whatever reason, "individuals or groups in a study differ systematically from the population of interest leading to a systematic error in an association or outcome" ([source](https://catalogofbias.org/biases/selection-bias/)). For example, selection bias might occur if you poll people entering a concert if they approve of higher taxes on the rich; they are more likely to be wealthy and thus say no.

A particularly interesting form of selection bias is *survivorship bias*; specifically, it is the error of presenting a subset of your data which has passed a particular test as representative of the entire set of data. A commonly discussed example of survivorship bias is from WWII ([source](https://en.wikipedia.org/wiki/Survivorship_bias#In_the_military)). The statistician Abraham Wald was tasked with deciding where to put armor on fighter planes for the U.S. Since more armor increases weight, decreasing range and maneuverability, Wald’s job was to find the most effective places to add armor. Luckily, the army had been gathering data on the distribution of bullet holes on planes. The distribution looked like this: 

<p align="center">
  <img width="550" src="https://upload.wikimedia.org/wikipedia/commons/9/98/Survivorship-bias.png">
</p>

The army suggested that the planes should be outfitted with armor on the areas which received the most fire, but Wald disagreed. Wald realized that the army could only collect data on planes *which made it back*; the fact that these returning planes didn’t have bullet holes on the cockpit, engines, middle wing, or back of the fuselage meant that planes that were shot there didn’t make it back. Upon his recommendation, the army added armor to parts of the plane with less bullet holes, a program that ended up being a massive success. 

### Anecdotal Evidence

One of the core reasons why anecdotal evidence is so troublesome is that, individually, there are a lot of confounding factors that only are resolved by averaging across a dataset that is truly representative of the general population. There are a few ways that this can cause issues. Firstly, ignoring these factors can lead to the post hoc fallacy: assuming a direct causal relationship from X to Y simply because X happened and then Y happened. 

Practically, using one person’s experience to try to justify a society-level policy is fallacious because the evidence is out on how *everyone else* would experience the policy. This means that in political science, anecdotes are absolutely worthless for anything but emotional warfare on their own. Unfortunately, they are an effective rhetorical device, so people (aware or not of the fallacy in their reasoning) will always use them to justify their positions. 

Finally, of course, there is the issue of selection bias. For example, any one person is more likely to share more extreme things that have happened to them. Thus, anecdotes will only spread if they are “surprising” (for example, perhaps people will only share their experiences with homeopathy if it substantially helped or hurt them). This can even lead to problems in science, where null results aren’t published because the individual scientists who found them deemed them not as “interesting” ([source](https://www.statnews.com/2017/11/10/null-research-findings/)). 

### The Texas Sharpshooter

The Texas Sharpshooter Fallacy refers to when a particular conclusion is drawn based on an incomplete set of the data (usually, a set of data that is purposefully picked to support one's claim). It comes from a story about a Texas gunman, who fired shots randomly into a wall. Afterwards, he found a cluster of shots close together, drew a target around it, and began to brag about his "sharp shooting" ([source](https://philosophyterms.com/the-texas-sharpshooter-fallacy/)). This fallacy occurs frequently during discussions about climate change, as people zoom in on graphs or focus on just one aspect of the weather ([source](https://www.climaterealityproject.org/blog/three-ways-climate-deniers-cherry-pick-facts-about-climate-change)). 

The way to avoid this fallacy is to avoid making claims that aren't supported by the *entirety* of the data. If you find data that contradicts your position, and ignore it without a good reason (such as methodological failure), then you may be subconsciously committing the Sharpshooter Fallacy. Similarly, if someone is making an argument that relates to a longer timeframe, larger population, or greater area than they cite, they may be actively selecting for data that agrees with them — and thus committing the Texas Sharpshooter Fallacy. 

### Why It's Important to Contextualize Data

[This study](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6039132/) discusses the important of contextualizing data. It tests the procedure and finds "modest gains by using a contextualized approach". However, more importantly, they argue that "neglecting contextual factors may lead to misdirected substantive conclusions, especially for older racial and ethnic minorities." They conclude that "to enhance the ecological validity of survey data, investigators should select theoretically-meaningful contextual data for specific research questions and consider cross-level interactions." In other words, understanding the context of the people surveyed can avoid making relevant errors such as the ecological fallacy. This can help avoid selection bias and bolster future research. It also allows for an appreciation of way in which situations impact different social groups individually. 

### Correlation and Causation
"Correlation is not equal to causation"; it's a phrase that is repeated at nearly every level of grade school, and there are plenty of articles that the hilarious effects of ignoring it (ex. [here](https://www.bloomberg.com/news/articles/2011-12-01/correlation-or-causation)). We won't waste too much time rehashing it here, but for the sake of completeness, we'll cover it briefly, because many political figures, regardless of ideology, still make this basic mistake. 

Essentially, a correlation between two variables *A* and *B* simply means that the quantity of *A* helps predict the quantity of *B*. It doesn't mean *A* causes *B*, *prima facie*; *B* could cause *A* or there could be no causal link at all. For example, both *A* and *B* could be caused by a third variable, *C*. In other words, a correlative link between *A* and *B* does not necessitate a causal link. 

To conclude that *A* causes *B*, scientists use Hill's Nine Criteria of Causation. The more criteria are met, the more likely a causal link is. 
1. **Strength:** there is a strong correlation between the supposed causes and the supposed effects.
2. **Consistency:** the result should be repeatable.
3. **Specificity:** other explanations have been ruled out.
4. **Temporality:** the supposed causes happen before the supposed effects.
5. **Gradient:** more of the cause induces more of the effect.
6. **Plausibility:** the explanation for casuality is plausible (though Hill admitted that sometimes science can't always give an explanation, so he viewed this as a weaker requirement).
7. **Coherence:** the proposed causal relationship is consistent with other causal relationships.
8. **Experimentally sound:** the proposed causal relationship has passed a randomized experiment.
9. **Analogous:** it's similar to another causal relationship. 
To learn more about Hill's Nine Criteria of Causation, visit [here](https://statisticsbyjim.com/basics/causation/). 

## The Pyschology of Changing Minds
### The Propagation of Misinformation
[Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018) is a study on the societal propagation of misinformation, the cognitive processes behind its persistence, and recommendations on its debunking. As the study says, "**it is a truism that a functioning democracy relies on an educated and well-informed populace"**. It also makes an important point to differ between a lack of information and *mis*information; "**ignorance rarely leads to strong support for a cause, in contrast to false beliefs based on misinformation, which are often held strongly and with (perhaps infectious) conviction.**" 

The study recognizes that there are many ways in which misinformation can be spread without any malign intent (for example, "during an evolving event or the updating of knowledge"), but also discusses four "other sources of misinformation that are arguably less benign". The first three are comparatively simple: 
1. **Rumors and myths.** Irrespective of political leaning, the study says, people tend to propagate information not on account of its truth or believability, but on the grounds that it will evoke an emotional response in the listener. This leads to the formation of myths solely for their "shock factor".
2. **Governments and politicians.** Politicians will often misinform the public for personal gain, either through blatant lying (Sarah Palin's "death panels" claim about Obamacare) or more subtle "linking" (the Bush administration implying it had evidence linking al-Qaida and Iraq to justify the Iraq war). Interestingly, though the public is aware that politicians mislead the public, research shows that "people are often unable to differentiate between information that is false and other information that is correct".
3. **Vested interests and nongovernmental organizations.** *Agnogenesis* refers to the "willful manufacture of mistaken beliefs". The study notes that there's "considerable legal and scientific evidence" that vested interests have engaged in this process "in at least two arenas — namely, industry-based responses to the health consequences of smoking and to climate change."

On the other hand, the fourth source discussed in the study, **the media**, deserves its own chapter. Here, we will briefly summarize the study's description of the ways the media misleds the public, but a further discussion of the problem and potential solutions are in the chapter "Media and Misinformation". 

The study finds that the media misinforms the public by oversimplifying, misrepresenting, or overdramatizing scientific results. This makes it critical to actually read cited studies. Even if you only have time to read the abstract, it's far better than nothing. The media also tends to try to present "both sides" of a story; while this is good in some cases, when one side of a story is founded on misinformation, balance can quickly become bias. Lewandowsky et al. cite [Boykoff and Boykoff 2004](https://www.sciencedirect.com/science/article/pii/S0959378003000669) as an example. Boykoff and Boykoff found that "**the prestige press's adherence to balance actually leads to biased coverage of both anthropogenic contributions to global warming and resultant action**", which in turn led to "**a significant divergence of popular discourse from scientific discourse**". 

The Internet, of course, is another important source of media misinformation, with the study citing research that has found that a substantial portion of online videos on important topics contain misinformation. It also notes the prevalence of websites solely dedicated to spreading hoaxes. Most interesting, perhaps, is that "**people who use new media, such as blogs, to source their news report that they find them fairer, more credible, and more in-depth than traditional sources**".

Finally, the study discusses "**increasing media fractionation**", noting that "the growth of cable TV, talk radio, and the Internet have made it easier for people to find news sources that support their existing views, a phenomenon known as *selective exposure*." This leads to the phenomenon of "**strategic extremism**", where the fractionated media landscape's ability to selective channel information to people that are likely to support it means that it's often in politicians' best interest to adopt extremist positions. 

### The Persistance of Misinformation

There are many factors leading to the surprising persistance of misinformation. Firstly, as noted by [Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018), "misleading information rarely comes with a warning label." The study discusses that "for better or worse, the acceptance of information as true is favored by tacit norms of everyday conversational conduct... **some research has even suggested that to comprehend a statement, people must at least temporarily accept it as true**." The study suggests that "although suspension of belief is possible", it's rare; "**the deck is stacked in favor of accepting information rather than rejecting it**". 

Another reason for the persistence of misinformation is the desire for a broader story, something that "lends sense and coherence to its individual elements", according to [Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018). The study argues that "**once a coherent story has been formed, it is highly resistant to change**" because "within the story, each element is supported by the fit of other elements, and any alteration of an element may be made implausible by the downstream inconsistencies it would cause." 

The study also notes that appeals to authority and popularity are common "shortcuts" that people use to judge the reliability of information. One way that this tendency can be taken advantage of is through simple reptition; [Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018) cites research finding that "the strongest predictor of belief in wartime rumors was simple repetition", and attributes this to **the ability of repitition to create the illusion of "social consensus even when no consensus exists.**"

Finally, there is also the issue of *confirmation basis*. As the study states, "as numerous studies in the literature on social judgment and persuasion have shown, **information is more likely to be accepted by people when it is consistent with other things they assume to be true.**" The force of an existing worldview is a powerful one: in [Bail et al. 2018](https://www.pnas.org/content/115/37/9216.full), researchers "conducted a field experiment that offered a large group of Democrats and Republicans financial compensation to follow bots that retweeted messages by elected officials and opinion leaders with opposing political views." They found that "**Republican participants expressed substantially more conservative views after following a liberal Twitter bot**". Similarly, the study notes that "Democrats’ attitudes became slightly more liberal after following a conservative Twitter bot" (although, as the study states, the Democrats' change was not statistically significant). 

**This process, by which exposing someone to opposing worldviews makes them more sure in their own, is known as *the backfire effect***, and is one of the main topics of [The Debunking Handbook](https://www.skepticalscience.com/docs/Debunking_Handbook.pdf), a helpful guide to the psychology of debunking myths derived from the study [Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018). The Debunking Handbook argues that the backfire effect is the product of various forces. Firstly, debunking a myth increases people's familiarity with an incorrect message; as discussed earlier, this repitition can reinforce their preconceptions. Secondly, when the "debunk" is more complicated than the myth, people's gravitation towards simple explanations can end up increasing their confidence in the misinformation. Thus, concise explanations (as long as they don't oversimplify) are key. Finally, of course, The Debunking Handbook discusses the impact of confirmation bias. The Debunking Handbook cites a study which found that **even when physically presented with a balanced set of facts, people will only absorb information that agrees with their preconceptions, thereby reinforcing their beliefs.**

[This New Yorker article](https://www.newyorker.com/magazine/2017/02/27/why-facts-dont-change-our-minds) and [this NPR podcast](https://www.npr.org/transcripts/743195213) are  two other pieces on the persistence of misinformation that, while they're less academic, are still interesting to read and listen to.

All of this can make correcting misinformation seem hopeless. However, now that we've acknowledged the existing difficulties, we're better prepared to discuss the research on how to overcome them.

### Tactics for Debunking Myths and Changing Minds

The study [Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018) (and [The Debunking Handbook](https://www.skepticalscience.com/docs/Debunking_Handbook.pdf) which is based on it) uses this graphic to demonstrate best practices when debunking myths: 

<p align="center">
  <img width="800" src="https://obiamal.github.io/political-research/images/debunking-misinformation.jpg">
</p>

In particular, the bottom three rows focus on ways to mitigate the three sources of the backfire effect. 

[Tan et al. 2016](https://dl.acm.org/doi/10.1145/2872427.2883081) ([accessible ArXiv version](https://arxiv.org/pdf/1602.01103v1.pdf)) analyzed the stylistic choices in successful and unsuccessful conversations on r/ChangeMyView, a subreddit where a user makes an original post asking others to change their view has a bot keep track of when a user's mind is changed in the comments. They found that, indeed, "stylistic choices in how the opinion is expressed carry predictive power." Some of the stylistic choices that they mentioned were: 
1. **Persuasive arguments tended to use the same [stopwords](https://www.ranks.nl/stopwords)** (words that don't contain content, such as articles, prepositions, and pronouns) as the original post. On the other hand, **persuasive arguments tended to use more *different* content words** than the original post; the study found this to be the single greatest sylistic predictor of an argument's persuasiveness. 
2. **Hedging (indicating uncertainty by saying things such as "it could be the case") is more common in persuasive arguments**; despite the fear that their presence signals a weaker argument, the authors of the study suggest "they may make an argument easier to accept by softening its tone".
3. The authors also found that the number of **"example markers"** such as “for example”, “for instance”, and “e.g.” is "significantly higher in persuasive arguments."
4. "Definite articles (e.g., “the” instead of “a”) are preferred, which suggests that **specificity is important in persuasive arguments.**"
5. The study found that "**persuasive arguments use a significantly larger absolute number of personal pronouns**", such as me, you, and us. 

[Feinberg and Willer 2015](https://journals.sagepub.com/doi/abs/10.1177/0146167215607842) discussed another interesting technique: **"moral reframing"**. They argue that moral rhetoric from liberals and conservatives is largely ineffective at swaying opponents to their side, suggesting that "advocates advancing these arguments fail to account for the divergent moral commitments that undergird America’s political divisions". In other words, the authors' hypothesis was that "**(a) political advocates spontaneously make arguments grounded in their own moral values, not the values of those targeted for persuasion, and (b) political arguments reframed to appeal to the moral values of those holding the opposing political position are typically more effective.**" They conducted six studies on diverse political issues ("including same-sex marriage, universal health care, military spending, and adopting English as the nation's official langauge") to test their hypothesis. Their first two studies validated part (a) of their hypothesis, and their last four validated part (b), suggesting that using moral reframing is a oft-forgotten but effective way to connect with and persuade political opponents. 

So what are the "conservative moral values" and "liberal moral values" that arguments should be reframed with? [Voelkel and Feinberg 2017](https://journals.sagepub.com/doi/full/10.1177/1948550617729408), another study on moral reframing, specifically discusses the academic consensus on the moral foundations for liberals and conservatives, saying that "**recently, researchers mapped the moral domain and found evidence for five moral foundations that form the basis of moral beliefs and judgments.**" These five foundations are as follows: 
1. "The harm/care foundation is concerned with other’s suffering and the need to prevent and alleviate such suffering."
2. "The fairness/cheating foundation relates to justice, equality, and discrimination." 
3. "The loyalty/betrayal foundation emphasizes the importance of one’s in-group and prioritizing that in-group."
4. "The authority/subversion foundation deals with respect for higher ranked individuals as well as adherence to tradition."
5. "Finally, the sanctity/degradation foundation is concerned with sacredness and purity and avoiding disgust-evoking behaviors."
The study noted that "**research has, in turn, found that compared to conservatives, liberals more strongly endorse the harm/care and the fairness/cheating foundations, while conservatives more strongly endorse the loyalty/betrayal, authority/subversion, and sanctity/degradation foundations.**"

Voelkel and Feinberg tested their theory with presidential candidates Donald Trump and Hillary Clinton. They found that "**conservatives reading a message opposing Donald Trump grounded in a more conservative value (loyalty) supported him less than conservatives reading a message grounded in more liberal concerns (fairness)**". Similarly, "liberals reading a message opposing Hillary Clinton appealing to fairness values were less supportive of Clinton than liberals in a loyalty-argument condition." They summarized their work by saying that "**these results highlight how moral reframing can be used to overcome the rigid stances partisans often hold and help develop political acceptance.**"

Another study which, if it ends up passing the peer-review process, will support the above research, is [Voelkel and Willer 2019](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3385818). In this study, Voelkel and Willer used the technique of moral reframing to make a progressive economic platform more attractive to conservative and moderate voters. They found that "**a presidential candidate who framed his progressive economic platform to be consistent with more conservative value concerns like patriotism, family, and respect for tradition – as opposed to more liberal value concerns like equality and social justice – was supported significantly more by conservatives and, unexpectedly, by moderates as well**." In particular, they noted that changing the progressiveness of the candidate's platform had "weak and inconsistent effects", demonstrating that (at least in their experiments) "framing mattered more than policy". They conclude by noting that "**moral reframing could be an effective alternative to policy centrism for candidates seeking broader support.**"

[Broockman and Kalla 2016](https://science.sciencemag.org/content/352/6282/220) provides some hope that bigotry can be combated with genuine conversations. In the study, Broockman and Kalla's 56 volunteers conducted a "door-to-door canvassing intervention in South Florida targeting antitransgender prejudice" with over 500 voters, using techniques such as "imploring individuals to actively take an outgroup’s perspective". They found **a substantial reduce in transphobia** that remained when they returned to check 3 months later. They also saw "increased support for a nondiscrimination law, even after exposing voters to counterarguments." As a result, the study's authors concluded that "**these findings suggest that it may be in campaigns’ own best interest to place renewed emphasis on a personal exchange of initially opposing views, even regarding controversial issues and across partisan lines.**"

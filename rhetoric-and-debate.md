# Rhetoric and Debate

---

* [Good- and Bad-Faith Debate](rhetoric-and-debate#good--and-bad-faith-debate)
* [Valid Argumentative Techniques](rhetoric-and-debate#valid-argumentative-techniques)
   * [Logical Steps](rhetoric-and-debate#logical-steps)
   * [Good-Faith Debate Tactics](rhetoric-and-debate#good-faith-debate-tactics)
* [Invalid Argumentative Techniques](rhetoric-and-debate#invalid-argumentative-techniques)
   * [Logical and Statistical Fallacies](rhetoric-and-debate#logical-and-statistical-fallacies)
   * [Bad-Faith Debate Tactics](rhetoric-and-debate#bad-faith-debate-tactics)
* [Substantiating Your Points](rhetoric-and-debate#substantiating-your-points)
   * [Appeals to Authority](rhetoric-and-debate#appeals-to-authority)
   * [Anecdotal Evidence and the Need for Data](rhetoric-and-debate#anecdotal-evidence-and-the-need-for-data)
   * [The Texas Sharpshooter](rhetoric-and-debate#the-texas-sharpshooter)
   * [Statistics vs. Studies](rhetoric-and-debate#statistics-vs-studies)
   * [Correlation and Causation](rhetoric-and-debate#correlation-and-causation)
* [The Pyschology of Changing Minds](rhetoric-and-debate#the-pyschology-of-changing-minds)
   * [The Propagation of Misinformation](rhetoric-and-debate#the-propagation-of-misinformation)
   * [The Persistance of Misinformation](rhetoric-and-debate#the-persistance-of-misinformation)
   * [Tactics for Debunking Myths and Changing Minds](rhetoric-and-debate#tactics-for-debunking-myths-and-changing-minds)

---

## Good- and Bad-Faith Debate

The following quote comes from John Reed, author of *How to Spot Dishonest Arguments* ([source](https://johntreed.com/blogs/john-t-reed-s-news-blog/60887299-intellectually-honest-and-intellectually-dishonest-debate-tactics)), and is worth considering: 

“There are only two intellectually-honest debate tactics:
1. pointing out errors or omissions in your opponent’s facts, and
2. pointing out errors or omissions in your opponent’s logic.” 

Similar is Paul Graham’s [Pyramid of Disagreement](http://www.paulgraham.com/disagree.html), which argues that there is a “hierarchy of disagreement”, where the higher-up forms of disagreement are more intellectually honest. Wikipedia contains [this helpful diagram](https://en.wikipedia.org/wiki/Paul_Graham_(programmer)#/media/File:Graham's_Hierarchy_of_Disagreement-en.svg) which summarizes Graham's points. The pyramid might seem obvious, but being conscious about your aims can help you stay focused in the heat of a debate.

<p align="center">
  <img width="600" src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Graham%27s_Hierarchy_of_Disagreement-en.svg/1280px-Graham%27s_Hierarchy_of_Disagreement-en.svg.png">
</p>

In short, a bad-faith actor is someone who does not enter a debate to learn or participate in a Socratic dialogue. It is someone who enters to “win” (usually, this means making themselves appear strong to the audience) and relies on invalid argumentative techniques (as discussed later) to do so. One red flag of a bad-faith actor is a lack of a consistent ideology; obvious contradictions usually means that the actor hasn’t actually grappled with critique. Yet it’s important to remember that calling someone a bad-faith actor to put them down instead of trying to better the conversation is itself bad-faith acting.

## Valid Argumentative Techniques

### Logical Steps

When trying to prove a statement *deductively* (with pure logic), one should rely on the [rules of inference](https://en.wikipedia.org/wiki/List_of_rules_of_inference), the [rules of replacement](https://en.wikipedia.org/wiki/Rule_of_replacement), and the [valid syllogistic forms](http://markmcintire.com/phil/validforms.html#). There is no need to list them out here: virtually all valid rules or forms are immediately intuitive, and almost all invalid rules or forms are obviously invalid (though some find the ideas of [existential import](http://cstl-cla.semo.edu/hhill/pl120/notes/existential%20import.htm) and the [existential fallacy](https://en.wikipedia.org/wiki/Existential_fallacy) genuinely confusing). One interesting technique in pure logic is proof by contradiction. This argumentative technique is also called *[reductio ad absurdum](https://iep.utm.edu/reductio/)*, and begins with the acceptance of a position, and the derivation of something absurd (in the logical sense). 

There is a fabulous quote by G.H. Hardy on this technique: “Reductio ad absurdum... is a far finer gambit then any chess gambit: a chess player may offer the sacrifice of a pawn or even a piece, but a mathematician offers the game” ([source](http://web.mnstate.edu/peil/geometry/Logic/6logic.htm)).

However, not all statements need to be proven deductively. *Inductive arguments* are arguments that do not prove their conclusion with total certainty, but rather argue the conclusion is extremely likely. Inductive arguments are not judged by their logical consistency, but by how "strong" or "weak" they are. For example, when done correctly, an argument from analogy can serve as inductive justification for a particular position. Of course, creating a false analogy is a fallacy, and determining what is “false” is hard, but this doesn’t mean that arguments from analogy are worthless. Another example of strong inductive reasoning is the process of using a large random sample of a population to reason about the entire population. [Here](https://examples.yourdictionary.com/examples-of-inductive-reasoning.html) are more examples of inductive reasoning.

### Good-Faith Debate Tactics

Here is a list of tactics that might not help you make your opponent “look bad”, but will ultimately help you convince them to change their mind. 
* *Stay calm.* Stereotypes about the overemotional abound; these stereotypes are undoubtedly harmful, but for the sake of optics it’s important to avoid them. 
* *Concede good points and point out when your opponent gets something right.* Dogmatically refusing to admit nuance in your conversation partner’s position (even when, to you, it seems like they have none) will only lock away the possibility for
* *Accept small changes.* You don’t need to compromise, but be prepared to accept small steps by your partner in the right direction. Ideology change does not happen all at once. 
* *Study your partner.* On one hand, understanding your opponent’s positions and tendencies will be helpful to prepare for their arguments or catch them in logical inconsistencies. On the other, it can help educate you on what your partner is missing or what arguments will be most effective.
* *Require a consistent ideology.* Tu quoque (discussed later) is a fallacy, but it is absolutely reasonable to require that each point of your partner’s ideology agrees with earlier points. 
* *Steel man your partner’s position.* Instead of trying to find a weak version of your opponent’s argument, find a strong version. Make their argument as effective as possible and restate it to them so that (1) you’re sure you understand them and (2) they know you’re listening to them. 

And, of course, avoid the fallacies and bad-faith debate tactics discussed later in this chapter. 

## Invalid Argumentative Techniques

### Logical and Statistical Fallacies

*Note:* I created this list myself, but used [this](https://yourlogicalfallacyis.com/), [this](https://www.steelonsteel.com/wp-content/uploads/2019/09/FOTW-2019-List-min.jpg), and [this site](https://thevisualcommunicationguy.com/wp-content/uploads/2015/02/Infographic_LogicalFallaciesCollection_LowRes.jpg) to make sure I didn’t miss any fallacies. 

*Argumentum ad hominem* (*ad hominem* for short) is a logical fallacy where the speaker tries to invalidate an argument by attacking the attributes of the argument’s maker rather than dealing with the argument itself. There are a few specific types of ad hominem arguments: 
* *Tu quoque* literally means “you also”, and refers to when the speaker tries to disprove an argument by attacking the history of the arguer. For example, a tu quoque fallacy occurs when the speaker tries to invalidate an argument on the basis that the arguer is a hypocrite.
* *Circumstantial ad hominem* is the fallacy of invalidating an argument on the basis that the arguer is biased. If properly justified, it is reasonable to invalidate empirical evidence due to the collector's bias, but the bias of the arguer never justifies the invalidation of an argument. 
* *Guilt by association*, *name-calling*, *the genetic fallacy*, and so on are all ad hominem fallacies. 

*Argumentum ad populum* (*ad populum* for short) is the logical fallacy of assuming that a proposition is true if most (or many) believe it, or conversely that a proposition is false if most (or many) don’t believe it. Ad populum is also called *appeal to popularity* or *the democratic fallacy*. On the other hand, there are times when appealing to popularity is valid; for example, elected officials in a representative democracy might look at polls to help determine what policies to implement. 

Related is the fallacious *appeal to authority*, where a speaker argues that a proposition is true solely because someone in a position of authority said so. However, despite the fact that this is a logical fallacy, the truth is that relying on academic evidence is often not just compelling, but necessary, to agree on certain base premises. In the section on substantiating points, we discuss appeals to authority — including how and why they can be done well — in more detail. 

*Affirming the consequent* is the logical fallacy of assuming that an implication implies its converse. More explicitly, this logical fallacy occurs when *A* implies *B*, and the speaker uses this to argue that if *B*, then *A*. For example, it is true that all apples are fruits, but would be fallacious to assume that any fruit necessarily be an apple. Equivalent is the logical fallacy of denying the antecedent.

Similar is the logical fallacy of *begging the question* or *circular reasoning*, where the speaker assumes a restated version of their proposition to prove it. For example, the statement “abortion is wrong because it is murder” begs the question because murder means “wrongful killing”. 

*The naturalistic fallacy* (also called *appeal to nature* or *appeal to tradition*) is the fallacy of assuming that because something is a certain way (or was a certain way, or has been a certain way) that it should be that way. Related is the *moralistic fallacy*, which essentially holds that some aspect of nature is undesirable or immoral and thus cannot exist. For example, I have heard on multiple occasions the argument that “X cannot exist, because God would not let such an awful thing happen.”

*Argumentum ad ignorantiam*, *the appeal to ignorance fallacy*, occurs when someone asserts that a proposition is true because they have seen (or there exists) no evidence to the contrary. This fallaciously shifts the burden of proof from the one making the claim to the one critiquing it, and thus is also called *the burden of proof fallacy*. In general, the burden of proof lies on the person making the claim (this is also known as *Russell’s teapot*). 

*The fallacy of composition* is related to *the fallacy of hasty generalization* and is where one concludes that something is true of an entire group from the fact that it’s true of some part of the whole. It is the converse of *the fallacy of division*, which is where one concludes that something is true of some part of a group from the fact that it applies to the whole group; it is essentially the problem with applying averages to individuals. For example, assuming that a particular person is more likely to be a criminal because they are part of a group which is more likely to be criminals. 

Related is *the ecological fallacy*, which generally happens when one confuses ecological correlations and individual correlations. This can manifest itself in using averages wrongly, in subsets’ behavior being contrary to the larger set’s behavior, and so on. One of the most famous examples of this was [Robinson’s paradox](https://www.jstor.org/stable/2087176), where Robinson found that there was a negative correlation between immigrants in an American state and its illiteracy rate. However, upon closer consideration of the data, he found that immigrants actually had a slightly higher illiteracy rate than the native population; they simply tended to move to states with high literacy rates. 

Another statistics-related fallacy is the confusion of *correlation vs. causation* (also known as non causa pro causa). In the above entry, we saw confusion between two types of correlations; in this fallacy, we note that just because *A* and *B* are correlated does not mean *A* causes *B*. It could be the case that *B* causes *A*, or that *A* and *B* are both caused by *C*, etc. For example, it is true that intelligence and wealth are correlated, but this does not necessarily mean intelligence causes wealth; an equally important realization is that having wealth (or, more accurately, not having to worry about poverty) actually makes one more intelligent (see the section on Class and Intelligence). 

A *false dichotomy* (also called a *black-and-white fallacy* or an *excluded middle fallacy*) occurs when a speaker tries to fallaciously cast a decision as having only two possible choices, when there may be a myriad of other possibilities. Conversely, the *middle ground fallacy* occurs when one assumes the compromise of two extremes must be the correct answer. 

The *slippery slope fallacy* occurs when one asserts that a small step leads to a chain of events leading to one a large (negative) event without sufficient justification. This is related to the *continuum fallacy*, which occurs when one ignores the possibility of a middle ground between two objects *A* and *B*. 

Finally, the *straw man fallacy* occurs when someone misrepresents their opponent’s argument in order to have an easier time refuting it. It is often used in conjunction with the slippery slope argument, wherein someone argues that an argument for *A* is really an argument for *B*, where the connection between *A* and *B* is provided by slippery-slope reasoning. 

Other important fallacies: 
* *appeal to emotion*, when an argument relies on the speaker’s emotions rather than logic
* *personal incredulity*, when a speaker invalidates a proposition due to personal disbelief
* *the gambler’s fallacy*, when a speaker tries to find a pattern in statistically independent events
* *the fallacy fallacy*, which is when a proposition is invalidated because a common argument for the proposition has a fallacy in it. 
* *loaded question/definition*, when a speaker defines something or asks a question in a biased manner. This is related to the straw man fallacy.

### Bad-Faith Debate Tactics

*The Gish gallop* is a bad-faith debate tactic where the arguer, instead of trying to have a two-way discussion, will try to overwhelm their opponent with point after point. Often, each of these points could be refuted if they were given individually, but together, it is difficult for the opponent to remember all of them, much less actually respond to them. 

*The Motte-and-Bailey tactic* occurs when an arguer holds a fairly controversial position (the “bailey”), and, upon being challenged, retreats to a much easier-to-defend position (the “motte”). By conflating the bailey with the motte, if their opponent cannot reject the motte, they can claim that they have proven the bailey. For example, a famous anti-intellectual motte-and-bailey argument uses the bailey “scientific knowledge cannot be trusted” but the motte “scientists have been wrong in the past”. 

Related is the tactic of *moving the goalposts*. Moving the goalposts occurs when a speaker holds a position justified by a fallacious argument, and when said argument is disproven, refuses to accept it, instead creating a new argument that justifies an easier-to-defend position. The new argument is often fallacious itself, and so the cycle can continue. The reason why this is considered a bad-faith tactic is because, if the speaker was trying to have a Socratic dialogue, they would begin with their actual position and the strongest argument for it. 

*Pivoting*, *a non sequitur*, or *a red herring* is a debate tactic/logical fallacy wherein the speaker will ignore the argument or proposition at hand, and make an entirely irrelevant point. Sometimes, this takes the form of ignoring an opponent’s argument and simply throwing out fallacious critiques of their conclusion. Other times, the speaker will simply change the subject instead of admitting defeat (presumably to insulate themselves from having to admit to being wrong). 

## Substantiating Your Points
### Appeals to Authority
### Anecdotal Evidence and the Need for Data
### The Texas Sharpshooter
### Statistics vs. Studies
### Correlation and Causation
## The Pyschology of Changing Minds
### The Propagation of Misinformation
[Lewandowsky et al. 2012](https://journals.sagepub.com/doi/full/10.1177/1529100612451018) is a study on the societal propagation of misinformation, the cognitive processes behind its persistence in individuals, and recommendations on its debunking. As the study says, "it is a truism that a functioning democracy relies on an educated and well-informed populace". It also makes an important point to differ between a lack of information and *mis*information; "ignorance rarely leads to strong support for a cause, in contrast to false beliefs based on misinformation, which are often held strongly and with (perhaps infectious) conviction." 

The study recognizes that there are many ways in which misinformation can be spread without any malign intent (for example, "during an evolving event or the updating of knowledge"), but also discusses four "other sources of misinformation that are arguably less benign". The first three are comparatively simple: 
**1. Rumors and myths.** Irrespective of political leaning, the study says, people tend to propagate information not on account of its believability or even its truth, but on the grounds that it will evoke an emotional response in the listener. This can be propagated through "urban myths" (such as "Bush did 9/11") or even through literary fiction. 
**2. Governments and politicians.** Politicians will often misinform the public for personal gain, either through blatant lying (Sarah Palin's "death panels" claim about Obamacare) or more subtle "linking" (the Bush administration implying it had evidence linking al-Qaida and Iraq to justify the Iraq war). Interestingly, though the public is aware that politicians mislead the public, research shows that "people are often unable to differentiate between information that is false and other information that is correct".
**3. Vested interests and nongovernmental organizations.** "Agnogenesis" refers to the "willful manufacture of mistaken beliefs". The study states that "there is considerable legal and scientific evidence for this process in at least two arenas—namely, industry-based responses to the health consequences of smoking and to climate change."

On the other hand, the fourth source discussed in the study, **the media**, deserves its own chapter (and has one). Here, we will briefly summarize the ways the study finds that the media misleds the public, but a further discussion of the problem and potential solutions in the chapter "Media and Misinformation". The study finds that the media misinforms the public by oversimplifying, misrepresenting, or overdramatizing scientific results; this makes it critical to *actually read cited studies* (even if it's just the abstract). The media also tends to try to present "both sides" of a story; while this is good in some cases, when one side of a story is founded on misinformation, balance can quickly become bias. The study cites [Boykoff and Boykoff 2004](https://www.sciencedirect.com/science/article/pii/S0959378003000669), which found that "the prestige press's adherence to balance actually leads to biased coverage of both anthropogenic contributions to global warming and resultant action", which led to "a significant divergence of popular discourse from scientific discourse". 

The Internet, of course, is an important source of misinformation, with the study citing research that has found a substantial portion of online videos on important topics contain misinformation and the prevalence of websites dedicated to sharing hoaxes. Most interesting, perhaps, is that "people who use new media, such as blogs, to source their news report that they find them fairer, more credible, and more in-depth than traditional sources"; this combined with the prevalence of misinformation on the Internet makes it a serious issue.

Finally, the study discusses "increasing media fractionation", noting that "the growth of cable TV, talk radio, and the Internet have made it easier for people to find news sources that support their existing views, a phenomenon known as *selective exposure*." This leads to the phenomenon of "strategic extremism", where the fractionated media landscape's ability to selective channel information to people that are likely to support it means that it's often in politicians' best interest to adopt extremist positions. 

### The Persistance of Misinformation
### Tactics for Debunking Myths and Changing Minds
